{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Orders Cleaning\n",
    "\n",
    "This notebook will perform basic cleaning of the `Orders` dataset in order to prepare for dashboarding and storytelling.\n",
    "\n",
    "Steps include:\n",
    "* Removing columns with no or single values.\n",
    "* Removing columns with no valuable information.\n",
    "* Standardizing item names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from belly_rubb.config import RAW_DATA_DIR, INTERIM_DATA_DIR\n",
    "from belly_rubb.utils import load_config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load orders\n",
    "\n",
    "orders_df = pd.read_csv(INTERIM_DATA_DIR / 'orders.csv')\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Standardizing Item Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load menu data\n",
    "\n",
    "catalog_df = pd.read_csv(RAW_DATA_DIR / 'MLW4W4RYAASNM_catalog-2025-08-26-2046.csv')\n",
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Normalize `Item Name` in orders\n",
    "\n",
    "- Lowercase\n",
    "- Trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(item: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize item names.\n",
    "\n",
    "    Change string to lowercase and trim whitespace surrounding the text.\n",
    "\n",
    "    Params:\n",
    "        item (str): Name of item.\n",
    "\n",
    "    Returns:\n",
    "        trimmed (str): Normalized text.\n",
    "    \"\"\"\n",
    "    lowercase = item.lower() # Lowercase name\n",
    "    trimmed = lowercase.strip() # Strip surrounding whitespace\n",
    "\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Item Name\n",
    "\n",
    "normalized_df = orders_df.dropna(subset='Item Name')\n",
    "normalized_df['Item Name'] = normalized_df['Item Name'].apply(lambda x: normalize(x))\n",
    "normalized_df['Item Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Load and generate mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data dictionary\n",
    "\n",
    "item_dict = load_config_file(file_path='../references/item_synonyms.json')\n",
    "print(item_dict is not None) # Confirm successfully loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping dictionary for item name and variation\n",
    "\n",
    "alias_to_base = {}\n",
    "var_map = {}\n",
    "\n",
    "# Loop through menu item dicts\n",
    "for base_name, cfg in item_dict.items():\n",
    "    # Loop through item name aliases\n",
    "    for alias in cfg['aliases']:\n",
    "        alias_to_base[alias.lower()] = base_name # {lowercase_alias: item_name}\n",
    "\n",
    "    # Loop through variations and their corresponding aliases\n",
    "    for var_base_name, var_alias_list in cfg['variations'].items():\n",
    "        # Loop through variation aliases\n",
    "        for var in var_alias_list:\n",
    "            var_map[var.lower()] = var_base_name # {lowercase_variation_alias: variation_base_name}\n",
    "\n",
    "alias_to_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Standardizing with `.map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map item name dictionary to Item Name\n",
    "\n",
    "normalized_df['std_names'] = normalized_df['Item Name'].map(alias_to_base)\n",
    "\n",
    "# View transformations\n",
    "normalized_df[['Item Name', 'std_names']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique standardized names\n",
    "\n",
    "normalized_df['std_names'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate Item Names of items with 'nan' standardized name\n",
    "\n",
    "normalized_df[normalized_df['std_names'].isna()]['Item Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate rows with null standardized name\n",
    "\n",
    "normalized_df[normalized_df['std_names'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Simply mapping aliases to base names does not capture items where the variation is included in `Item Name`. A more manual approach is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Standardizing with `.apply()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paren_text(text: str, start_str: str, end_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text between two strings not including the strings.\n",
    "\n",
    "    Uses indeces of start_str and end_str to extract text between them.\n",
    "\n",
    "    Params:\n",
    "        text (str): Text to search within.\n",
    "        start_str (str): Starting character.\n",
    "        end_str (str): Ending character.\n",
    "\n",
    "    Returns:\n",
    "        str: Text between start_str and end_str\n",
    "    \"\"\"\n",
    "    # Find index of start_str\n",
    "    start_idx = text.find(start_str) + 1\n",
    "    if start_idx == -1: # Check if found\n",
    "        # print(f\"Start string {start_str} not found in {text}.\")\n",
    "        return None\n",
    "    \n",
    "    # Find index of end_str\n",
    "    end_idx = text.find(end_str, start_idx)\n",
    "    if end_idx == -1: # Check if found\n",
    "        # print(f\"End string {end_str} not found in {text}.\")\n",
    "        return None\n",
    "    \n",
    "    return text[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_name_vars(row: pd.Series):\n",
    "    \"\"\"\n",
    "    Standardizes Item name\n",
    "    \"\"\"\n",
    "    item_name = row['Item Name'].lower()\n",
    "    item_variation = row['Item Variation'].lower()\n",
    "\n",
    "    # Check if item name is in aliases\n",
    "    if item_name in alias_to_base:\n",
    "        row['std_name'] = alias_to_base.get(item_name, item_name) # Save standardized name to std_name\n",
    "        row['std_variation'] = var_map.get(item_variation, item_variation) # Save standardized variation to std_variation\n",
    "    else:\n",
    "        # Check if variation in item name in parentheses\n",
    "        var = paren_text(text=item_name, start_str='(', end_str=')')\n",
    "\n",
    "        # If variation found between parentheses\n",
    "        if var:\n",
    "            # Subtract variation from item name\n",
    "            item_name = re.sub(r\"\\((.*?)\\)\", \"\", string=item_name).strip().lower() # Extract name from parentheses\n",
    "\n",
    "            # Save standardized names, default to 'na'\n",
    "            row['std_name'] = alias_to_base.get(item_name, item_name)\n",
    "            row['std_variation'] = var_map.get(var, item_variation)\n",
    "        else:\n",
    "            # Item name doesn't include variation name in parentheses\n",
    "            row['std_name'] = alias_to_base.get(item_name, item_name)\n",
    "            row['std_variation'] = var_map.get(var, item_variation)\n",
    "            \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null Item name values and apply standardize_name_vars\n",
    "\n",
    "no_null_df = orders_df.dropna(subset='Item Name')\n",
    "standardized_df = no_null_df.apply(standardize_name_vars, axis=1)\n",
    "\n",
    "standardized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows with na standardized variation\n",
    "\n",
    "null_std_var = standardized_df[standardized_df['std_variation'] == 'na']\n",
    "null_std_var[['Item Name', 'Item Variation']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_std_var['Item Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "There appear to be `Item Name` and `Item Variation` entries that were not included in the data dictionary. These will be further explored here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Item Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "##### Belly Sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "belly_slider_orders = standardized_df[standardized_df['Item Name'] == 'BELLY SLIDERS']\n",
    "belly_slider_orders.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "belly_slider_orders['std_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "belly_slider_orders['Item Variation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "belly_slider_orders['Order Date'].agg(['max', 'min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "###### Conclusion\n",
    "\n",
    "* Orders of `BELLY SLIDERS` were not added to the data dictionary.\n",
    "* They do not exist in current menu.\n",
    "\n",
    "**Resolution:** Added to dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "##### Slow-Cooked Asparagus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "asparagus_orders = standardized_df[standardized_df['Item Name'] == 'SLOW-COOKED ASPARAGUS']\n",
    "asparagus_orders.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "asparagus_orders['std_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "asparagus_orders['Order Date'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "asparagus_orders['Item Variation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "asparagus_orders['std_variation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "###### Conclusion\n",
    "\n",
    "* `SLOW-COOKED ASPARAGUS` not part of data dictionary.\n",
    "* Not in current menu version.\n",
    "\n",
    "**Resolution:** Added to data dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "##### Marsh'n'Cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_df = standardized_df[standardized_df['Item Name'] == 'MARSH’n’COOKIE']\n",
    "cookie_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_df['std_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_df['Item Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_df['Item Variation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_df[['std_name', 'std_variation']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "###### Conclusions\n",
    "\n",
    "* Entry in data dictionary was misspelled.\n",
    "\n",
    "**Resolution:** Spelling corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "##### Mozzarella Wedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mozz_wedge = standardized_df[standardized_df['Item Name'] == 'MOZZARELLA WEDGES']\n",
    "mozz_wedge.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mozz_wedge['Order Date'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mozz_wedge['Item Variation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "###### Conclusions\n",
    "* `MOZZARELLA WEDGES` were not in the data dictionary.\n",
    "* Since there are only **8** orders, and `Item Variations` are equivalent to variations for oders of *Mozzarella sticks*, this will be added as an alias for **mozzarella sticks**.\n",
    "\n",
    "**Resolution:** Added to list of *aliases* for **mozzarella sticks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "##### Pork Noodle Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_df = standardized_df[standardized_df['Item Name'] == 'PORK NOODLE SOUP']\n",
    "soup_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_df['Order Date'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "###### Conclusions\n",
    "\n",
    "* There is only **1** order of pork noodle soup.\n",
    "* Since it also is not in the current menu, it will be dropped.\n",
    "\n",
    "**Next steps:** Add cleaning step to drop items not in the data dictinoary with only 1 order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "##### Chop Chop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "chop_df = standardized_df[standardized_df['Item Name'] == 'CHOP-CHOP']\n",
    "chop_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "chop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "###### Conclusions\n",
    "* Similar to *Pork Noodle Soup*, there is only **1** order of `CHOP-CHOP` and it is not part of the menu catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "##### Local Delivery Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = standardized_df[standardized_df['Item Name'] == 'Local Delivery Service']\n",
    "delivery_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "###### Conclusion\n",
    "* Similar to *Pork Noodle Soup* and *Chop-Chop*.\n",
    "* It will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Party Package !BEEF RIBS ONLY! (4-6 ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prty_df = standardized_df[standardized_df['Item Name'] == 'Party Package !BEEF RIBS ONLY! (4-6 ppl)']\n",
    "prty_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prty_df[['std_name', 'std_variation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "###### Conclusion\n",
    "\n",
    "* Only **1** order, therefore it will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### Dropping Single Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of items with only 1 order\n",
    "\n",
    "item_name_counts = standardized_df['Item Name'].value_counts(ascending=True).reset_index()\n",
    "single_item_names = item_name_counts[item_name_counts['count'] == 1]['Item Name'].values\n",
    "\n",
    "single_item_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop orders with single-order Item Name\n",
    "\n",
    "single_orders = standardized_df[standardized_df['Item Name'].isin(single_item_names)]\n",
    "no_single_orders_df = standardized_df.drop(index=single_orders.index)\n",
    "\n",
    "any(no_single_orders_df['Item Name'].isin(single_item_names)) # Validate resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List remaining unstandardized Item Names\n",
    "\n",
    "no_single_orders_df[no_single_orders_df['std_name'] == 'na']['Item Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_single_orders_df[no_single_orders_df['std_variation'] == 'na']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "This section validates the results of standardizing with the above applied methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View unique combinations of Item name and standardized name\n",
    "\n",
    "no_single_orders_df[['Item Name', 'std_name']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View unique combinations of item variation and standardized variation\n",
    "\n",
    "no_single_orders_df[['Item Variation', 'std_variation']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fuzzy matching to evaluate std_name\n",
    "\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "def name_is_fuzzy_match(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Check if Item name and std_name are a fuzzy match.\n",
    "\n",
    "    Uses threshold of 80 to evaluate match. Adds column name_fuzzy_match with True for fuzzy matches\n",
    "    and False otherwise.\n",
    "\n",
    "    Params:\n",
    "        row (Series): Row from data.\n",
    "\n",
    "    Returns:\n",
    "        row (Series): Row with added column name_fuzzy_match.\n",
    "    \"\"\"\n",
    "    row['name_fuzzy_match'] = token_set_ratio(row['Item Name'].lower(), row['std_name'].lower()) > 80\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fuzzy matching to evaluate std_variation\n",
    "\n",
    "def variation_is_fuzzy_match(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Check if Item variation and std_variation are a fuzzy match.\n",
    "\n",
    "    Uses threshold of 80 to evaluate match. Adds column variation_fuzzy_match with True for fuzzy matches\n",
    "    and False otherwise.\n",
    "\n",
    "    Params:\n",
    "        row (Series): Row from data.\n",
    "\n",
    "    Returns:\n",
    "        row (Series): Row with added column variation_fuzzy_match.\n",
    "    \"\"\"\n",
    "    row['variation_fuzzy_match'] = token_set_ratio(row['Item Variation'].lower(), row['std_variation'].lower()) > 80\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fuzzy match results for std_name\n",
    "\n",
    "name_fuzzy_df = no_single_orders_df.apply(name_is_fuzzy_match, axis=1)\n",
    "false_match = name_fuzzy_df[name_fuzzy_df['name_fuzzy_match'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View unique combinations where not a fuzzy match\n",
    "\n",
    "false_match[['Item Name', 'std_name']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "Although these instances are not a fuzzy match, they are still **valid**. They signify items that experienced a drastic name change when updating the menu catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate standardized variations\n",
    "\n",
    "variation_fuzzy_df = name_fuzzy_df.apply(variation_is_fuzzy_match, axis=1)\n",
    "variation_fuzzy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique combinations where not a fuzzy match\n",
    "\n",
    "false_match = variation_fuzzy_df[variation_fuzzy_df['variation_fuzzy_match'] == False]\n",
    "false_match[['Item Variation', 'std_variation']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further inspect items with Regular item variation and full rack standardized\n",
    "\n",
    "regular_full = false_match[(false_match['Item Variation'] == 'Regular') & (false_match['std_variation'] == 'full rack')]\n",
    "regular_full['Item Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "* Instances where *Regular* turned into *full rack* were orders where the item variation was a *part of* the item name.\n",
    "* Other failed fuzzy matches are **valid** and signify major updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "# Data Quality\n",
    "These cleaning steps will include:\n",
    "* [x] Dropping static columns.\n",
    "* [x] Dropping fully empty features.\n",
    "* [ ] Dropping insignificant columns:\n",
    "    * `Fullfillment Notes`\n",
    "    * `Recipient Country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Item Name and Item Variation columns with standardized values\n",
    "no_single_orders_df['Item Name'] = no_single_orders_df['std_name']\n",
    "no_single_orders_df['Item Variation'] = no_single_orders_df['std_variation']\n",
    "\n",
    "# Drop std cols\n",
    "no_single_orders_df = no_single_orders_df.drop(labels=['std_name', 'std_variation'], axis=1)\n",
    "no_single_orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "## Static Columns & Empty Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop static columns\n",
    "\n",
    "clean_df = no_single_orders_df.drop(no_single_orders_df.columns[no_single_orders_df.nunique() < 2].values, axis=1)\n",
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "This step also drops empty features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "## Empty Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm no empty features\n",
    "\n",
    "clean_df.columns[clean_df.isna().all()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "## Insignificant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Fulfillment Notes', 'Recipient Country']\n",
    "\n",
    "clean_df = clean_df.drop(labels=cols, axis=1)\n",
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The preceding cleaning steps are intended to prepare the data for EDA and visualizations.\n",
    "* Unimputed missing values remain.\n",
    "* Features with high levels of missing values remain.\n",
    "* Rows have not been aggregated on the order level.\n",
    "* Features have not been engineered or transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df\n",
    "\n",
    "clean_df.to_csv(INTERIM_DATA_DIR / 'orders_eda_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belly_rubb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
